{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "np.random.seed(31415) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Machine Learning Classification\n",
    "Now that we have a sense of what our data loooks like, we're ready to build our classifier. We'll do this in three steps:\n",
    "\n",
    "* 1. **Data Cleanup**: We'll take a few extra steps to clean up our data and prepare it for our classifiers\n",
    "* 2. **Training**: We'll train several machine learning models on our input data\n",
    "* 3. **Evaluation/Analysis**: We'll look at the results of our classifiers and analyze them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df):\n",
    "    \"\"\"\n",
    "    Separates the dataset into X and y\n",
    "    \"\"\"\n",
    "    X = df.loc[:, df.columns != 'Outcome']\n",
    "    y = df.Outcome\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = prepare_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup\n",
    "No dataset is perfect. When you look at some of our features, you might notice that not everything makes sense. For example, if you look at the minimum of some of these columns, you notice that some patients have a BMI and blood pressure of 0. Does that sound right?\n",
    "\n",
    "Chances are these are **missing values**: those patients don't really have a BMI of 0, but maybe the researchers didn't collect those patient's BMI and so just put 0 in as a subsitute. \n",
    "You might also see these as \"NaN\", meaning \"not a number\", but in this case they were assigned a value of 0.\n",
    "\n",
    "## Questions to discuss\n",
    "- Why might these values be missing?\n",
    "- Does every column with a \"0\" mean that that's a missing value?\n",
    "- What are some potential problems of building a classifier with missing values?\n",
    "- What should we do about them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to handle missing values. To keep things simple, we'll simply replace those \"0\" values with the average of that column, which is a rough estimate of what we might expect that patient to have. This is called [data imputation](https://www.theanalysisfactor.com/seven-ways-to-make-up-data-common-methods-to-imputing-missing-data/).\n",
    "\n",
    "\n",
    "Not every column has missing values - for example, 0 pregnancies makes sense, and the minimum age is 21. We'll only impute the values in the columns for which a value of \"0\" doesn't make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_fill = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\n",
    "X[columns_to_fill] = X[columns_to_fill].replace(0, X[columns_to_fill].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step we'll take is **scaling** the data. Each of the columns of our table has very different ranges. Some algorithms expect each variable to be scaled within a normal range. \n",
    "\n",
    "https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler\n",
    "https://en.wikipedia.org/wiki/Feature_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X = pd.DataFrame(data=X, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = X.hist(figsize=(10,8), grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We're now finally ready to train our models! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test-Train Split\n",
    "We'll also split up our dataset into a *train* and *test* set. Our ultimate goal is to be able to predict whether a set of brand-new patients has diabetes. These new patients have never been seen before by our classifier. \n",
    "\n",
    "A common practice in machine learning development is to take a portion of our data and leave those rows out during training, then we'll see how our classifiers perform on these rows.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets\n",
    "\n",
    "## Questions to discuss\n",
    "- Why is it important  to evaluate on testing data that is separate from our training data?\n",
    "- How should you select the data that you'll leave out for testing?\n",
    "- What are the costs of taking out data for testing? ie., making a **blind testset**\n",
    "- What proportions should be used for testing and training? Is there a guiding principle? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing an algorithm\n",
    "There are many different types of algorithms that can be used for machine learning classification. Each one works a little differently and some work better for a specific problem. To find the best one, we'll pick a few different models and train each of them, then analyze and compare the results.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Outline_of_machine_learning#Types_of_machine_learning_algorithms\n",
    "We won't go into the details about each classifier, but we'll try out each of these 6 classifiers. Feel free to look them up and see how each performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"LR\": LogisticRegression(),\n",
    "          \"NB\": GaussianNB(),\n",
    "          \"KNN\": KNeighborsClassifier(),\n",
    "          \"DT\": DecisionTreeClassifier(),\n",
    "          \"RFC\": RandomForestClassifier(),\n",
    "          \"SVM\": SVC()\n",
    "         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = []\n",
    "model_names_scores = []\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    print(\"Training {}\".format(name))\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    pred = model.predict(X_test)\n",
    "    # Measure the accuracy\n",
    "    accuracy  = accuracy_score(y_test, pred)\n",
    "    print(\"Accuracy: {}\".format(accuracy))\n",
    "    print()\n",
    "    \n",
    "    # Append to this list to analyze later\n",
    "    model_names_scores.append((name, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Analysis\n",
    "Let's see how our classifiers did on our test set. Let's start by sorting the scores by accuracy and plotting their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_name_scores = sorted(model_names_scores, key=lambda x:x[1], reverse=True)\n",
    "sorted_names, sorted_scores = zip(*sorted_name_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = range(len(sorted_names))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(x_plot, sorted_scores)\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "\n",
    "ax.set_xticks(x_plot)\n",
    "_ = ax.set_xticklabels(sorted_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closer analysis\n",
    "Now, let's take the best-performing model and look at more details. While training, we only looked at the **accuracy** of the classifier. But an accuracy score is sometimes insufficient. **Question**: Can you think of some reasons why?\n",
    "\n",
    "To supplement this, we'll look at three more metrics: **precision**, **recall**, and **f1-score**. Here is a blog post that explains the difference between these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\n",
    "\n",
    "In summary, here's what each metric tells us:\n",
    "- **Precision**: If our classifier says a patient has diabetes, how likely is it that our classifier is correct?\n",
    "- **Recall**: If our classifier is given a patient with diabetes, how likely is it that our classifier will correctly predict that?\n",
    "- **F1**: This is a balanced average of the two\n",
    "\n",
    "#### Now, analyze your results\n",
    "Look at the graph and take name of the highest-performing classifier (the first label on the x-axis). Replace the value of `best_clf_name` in the cell below. We'll then look at more detailed metrics for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the best score\n",
    "best_clf_name = sorted_names[0]\n",
    "best_clf = models[best_clf_name]\n",
    "print('Best Model: {0}'.format(best_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf.fit(X_train, y_train) # Retrain\n",
    "pred = best_clf.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions to discuss\n",
    "- Compare your results to the results of others in your group. Did you get the same results, or are they different?\n",
    "- What kind of factors can cause you to get different results from someone else?\n",
    "- If you want, run this part of the notebook, starting at the cell that says \"Split up Data\". Try changing the parameter `test_size` (currently 0.2). Does doing this change your results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "Let's now plot a **confusion matrix** to see what kinds of mistakes our classifier is making. A confusion matrix plots the **truth** on the y axis (whether a patient had diabetes or didn't) against the **predictions** on the x axis (whether or not our classifier predicted diabetes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [\"No Diabetes\",\"Diabetes\"]\n",
    "ax = sns.heatmap(conf, annot=True, xticklabels=label, yticklabels=label)\n",
    "ax.set_ylabel('Truth', fontdict={'size': 20})\n",
    "_ = ax.set_xlabel('Prediction', fontdict={'size': 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions to discuss\n",
    "- What does the confusion matrix below tell us? What is our classifier doing a good job at doing? What does it not do as well at?\n",
    "- What type of error is the worst? Does this depend on the situation that you're using the classifier for?\n",
    "- What do we do about these errors? Compare that to a rule-based classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Next?\n",
    "This was a quick overview of the machine learning process. Here are some additional steps you could take to improve your classifier:\n",
    "- **Model tuning**: We tested a few different classifiers, but we could make a lot of changes to each classifier. Machine learning models have *hyperparameters* which are values that can be tweaked to optimize the performance of the classifier\n",
    "- **Additional data transformations**: We replaced missing values and scaled the data. We could do additional work to clean up and transform the data\n",
    "- **Try a larger dataset**: This dataset has only 784 patients. Machine learning benefits from having **lots of data** so that it can see lots of examples of patients to learn patterns from. This is a relatively small dataset. If there are larger, similar datasets that fit our needs, you may want to try and see if using a larger training set improves your performance\n",
    "- **Cross-Validate**: We split our data into a *training set* and a *testing set*. What was the reason for this? What are some disadvantages of doing this? Cross-validation is another way of splitting up our data in a way that lets you train and test on every patient rather than leaving 20% out of the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
